<h2 align="center">FGSM_Report</h2>

#### 1. **实验目的**

本次实验的目的是通过实现并应用 FGSM（Fast Gradient Sign Method）对抗攻击，探究对抗样本对深度学习模型（LeNet-5）的影响。实验目标如下：

- **Checkpoint 1**：在 MNIST 数据集上训练 LeNet-5，并成功实施 FGSM 对抗攻击，使得模型对某些样本进行错误分类。
- **Checkpoint 2**：通过调整攻击策略，生成定向对抗样本，使得模型将某一特定样本错误分类为“9”。

#### 2. **实验步骤**

1. **数据准备与预处理**：
   - 使用 PyTorch 提供的 `torchvision.datasets.MNIST` 数据集加载训练和测试数据。
   - 对图像进行简单的标准化处理，使得每张图像的像素值范围在 [0, 1] 之间。
2. **模型构建与训练**：
   - 使用 LeNet-5 架构作为分类模型。该模型由两层卷积层和三层全连接层构成，最后通过 softmax 层输出分类结果。
   - 在 MNIST 数据集上训练 LeNet-5，使用交叉熵损失函数（`CrossEntropyLoss`）和 SGD 优化器（`SGD`）。
3. **FGSM 对抗攻击的实现**：
   - 实现了 FGSM 对抗攻击算法。通过计算损失函数对输入数据的梯度，并按照梯度的符号方向对图像进行微小扰动，生成对抗样本。
   - `epsilon` 是攻击的强度，通过调整其大小来控制扰动的强度。我们使用了 `epsilon = 0.4` 来进行攻击。
4. **定向对抗样本生成**：
   - 为了生成定向攻击样本，使得模型将目标样本错误分类为“9”，我们对目标类别（即“9”）进行了定向设置。
   - 通过控制损失函数的目标标签，在生成对抗样本时，让其朝着特定标签“9”偏移。
5. **实验结果**：
   - 在原始样本上，LeNet-5 成功地将标签为“4”的数字分类正确。
   - 通过 FGSM 攻击后，模型错误分类为“9”，展示了对抗攻击的有效性。